set.seed(467)
austin_rs <- tune_grid(
austin_wf,
resamples = word_folds,
lambda_grid
)
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
austin_wf <- workflow() %>%
add_model(lasso_spec) %>%
add_recipe(austin_rec, blueprint = sparse_bp)
austin_wf
lambda_grid <- grid_regular(penalty(range = c(-3, 0)), levels = 20)
library(textrecipes)
austin_rec <- recipe(priceRange ~ description, data = train_words) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 1000) %>%
step_rm(description, pattern = "\\b[1-5]\\b") %>%
step_tfidf(description)
sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
austin_wf <- workflow() %>%
add_model(lasso_spec) %>%
add_recipe(austin_rec, blueprint = sparse_bp)
austin_wf
lambda_grid <- grid_regular(penalty(range = c(-3, 0)), levels = 20)
doParallel::registerDoParallel()
set.seed(467)
austin_rs <- tune_grid(
austin_wf,
resamples = word_folds,
lambda_grid
)
austin_rs
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
train_raw <- read.csv("data/train.csv")
price_plot <- train_raw %>%
mutate(priceRange = parse_number(priceRange)) %>%
ggplot(aes(longitude, latitude, z = priceRange)) +
stat_summary_hex(alpha = 0.8, bins = 50) +
scale_fill_viridis_c() +
labs( fill = "mean", title = "Price" )
price_plot
library(patchwork)
plot_austin <- function(var, title)
train_raw %>%
ggplot(aes(longitude, latitude, z = {{var}} )) +
stat_summary_hex(alpha = 0.8, bins = 50) +
scale_fill_viridis_c() +
labs( fill = "mean", title = title )
(price_plot + plot_austin(avgSchoolRating, "School Rating")) /
(plot_austin(yearBuilt, "Year Built") + plot_austin(log(lotSizeSqFt), "Lot Size"))
austin_tidy <- train_raw %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords())
austin_tidy %>%
count(word, sort = TRUE)
austin_tidy
top_words <- austin_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(1:100000),
!word %>% str_detect("\\d"),
!word == "w") %>%
slice_max(n, n = 500) %>%
pull(word)
top_words
word_freq <- austin_tidy %>%
count(word, priceRange) %>%
complete(word, priceRange, fill = list(n = 0)) %>%
group_by(priceRange) %>%
mutate(price_total = sum(n),
propotion = n / price_total) %>%
ungroup() %>%
filter(word %in% top_words)
word_freq
word_word_cloud <- word_freq %>%
filter(word %in% higher_words)  %>%
filter(priceRange == 750000) %>%
select(-priceRange, -price_total, -propotion) %>%
mutate(class = "high_words") %>%
rbind(
word_freq %>%
filter(word %in% lower_words)  %>%
filter(priceRange == 100000) %>%
select(-priceRange, -price_total, -propotion) %>%
mutate(class = "low_words")
)
library(tidyverse)
library(tidytext)
library(tidymodels)
library(patchwork)
library(ggwordcloud)
train_raw <- read.csv("data/train.csv")
train_raw <- as_tibble(train_raw)
train_raw
price_plot <- train_raw %>%
mutate(priceRange = parse_number(priceRange)) %>%
ggplot(aes(longitude, latitude, z = priceRange)) +
stat_summary_hex(alpha = 0.8, bins = 50) +
scale_fill_viridis_c() +
labs( fill = "mean", title = "Price" )
price_plot
plot_austin <- function(var, title)
train_raw %>%
ggplot(aes(longitude, latitude, z = {{var}} )) +
stat_summary_hex(alpha = 0.8, bins = 50) +
scale_fill_viridis_c() +
labs( fill = "mean", title = title )
(price_plot + plot_austin(avgSchoolRating, "School Rating")) /
(plot_austin(yearBuilt, "Year Built") + plot_austin(log(lotSizeSqFt), "Lot Size"))
austin_tidy <- train_raw %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords())
austin_tidy %>%
count(word, sort = TRUE)
austin_tidy
top_words <- austin_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(1:100000),
!word == "w") %>%
slice_max(n, n = 1000) %>%
pull(word)
word_freq <- austin_tidy %>%
count(word, priceRange) %>%
complete(word, priceRange, fill = list(n = 0)) %>%
group_by(priceRange) %>%
mutate(price_total = sum(n),
propotion = n / price_total) %>%
ungroup() %>%
filter(word %in% top_words)
word_freq
word_mods <- word_freq %>%
nest(data = -word) %>%
mutate(model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, .,
family = "binomial")),
model = map(model, tidy)) %>%
unnest(model) %>%
filter(term == "priceRange") %>%
mutate(p.value = p.adjust(p.value)) %>%
arrange(estimate)
word_mods
higher_words <- word_mods %>%
filter(p.value < 0.05) %>%
slice_max(estimate , n = 50) %>%
pull(word)
higher_words
lower_words <- word_mods %>%
filter(p.value < 0.05) %>%
slice_max(-estimate , n = 50) %>%
pull(word)
lower_words
word_word_cloud <- word_freq %>%
filter(word %in% higher_words)  %>%
filter(priceRange == 750000) %>%
select(-priceRange, -price_total, -propotion) %>%
mutate(class = "high_words") %>%
rbind(
word_freq %>%
filter(word %in% lower_words)  %>%
filter(priceRange == 100000) %>%
select(-priceRange, -price_total, -propotion) %>%
mutate(class = "low_words")
)
word_word_cloud
word_word_cloud %>%
mutate(class = factor(class)) %>%
ggplot(aes(label = word, color = class, size = n)) +
geom_text_wordcloud_area() +
facet_wrap(~class) +
scale_size_area(max_size = 16)
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
set.seed(236)
word_data <- austin_data %>%
initial_split(strata = priceRange)
word_data
train_words <- training(word_data)
test_words <- testing(word_data)
set.seed(124)
word_folds <- vfold_cv(train_words, strata = priceRange)
library(textrecipes)
austin_rec <- recipe(priceRange ~ description, data = train_words) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 1000) %>%
step_rm(description, pattern = "\\b[1-5]\\b") %>%
step_tfidf(description)
sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
austin_wf <- workflow() %>%
add_model(lasso_spec) %>%
add_recipe(austin_rec, blueprint = sparse_bp)
austin_wf
lambda_grid <- grid_regular(penalty(range = c(-3, 0)), levels = 20)
doParallel::registerDoParallel()
set.seed(467)
austin_rs <- tune_grid(
austin_wf,
resamples = word_folds,
lambda_grid
)
austin_rs
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
set.seed(236)
word_data <- austin_data %>%
initial_split(strata = priceRange)
word_data
train_words <- training(word_data)
test_words <- testing(word_data)
set.seed(124)
word_folds <- vfold_cv(train_words, strata = priceRange)
library(textrecipes)
austin_rec <- recipe(priceRange ~ description, data = train_words) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 1000) %>%
step_tfidf(description)
sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
austin_wf <- workflow() %>%
add_model(lasso_spec) %>%
add_recipe(austin_rec, blueprint = sparse_bp)
austin_wf
lambda_grid <- grid_regular(penalty(range = c(-3, 0)), levels = 20)
doParallel::registerDoParallel()
set.seed(467)
austin_rs <- tune_grid(
austin_wf,
resamples = word_folds,
lambda_grid
)
austin_rs
show_best(austin_rs, "rmse")
austin_rs
# Show the best results based on RMSE
show_best(austin_rs, metric = "rmse")
show_best(austin_rs, metric = "rmse")
austin_rec
austin_rec %>%
prep()
prep(austin_rec)
prep(austin_rec) %>%
juice()
prep(austin_rec) %>%
juice()
prep(austin_rec) %>%
juice()
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
set.seed(236)
word_data <- austin_data %>%
initial_split(strata = priceRange)
word_data
train_words <- training(word_data)
test_words <- testing(word_data)
set.seed(124)
word_folds <- vfold_cv(train_words, strata = priceRange)
library(textrecipes)
austin_rec <- recipe(priceRange ~ description, data = train_words) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 1000) %>%
step_tfidf(description)
sparse_bp <- hardhat::default_recipe_blueprint(composition = "dgCMatrix")
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
austin_wf <- workflow() %>%
add_model(lasso_spec) %>%
add_recipe(austin_rec, blueprint = sparse_bp)
austin_wf
lambda_grid <- grid_regular(penalty(range = c(-3, 0)), levels = 20)
doParallel::registerDoParallel()
set.seed(467)
austin_rs <- tune_grid(
austin_wf,
resamples = word_folds,
lambda_grid
)
austin_rs
# Show the best results based on RMSE
show_best(austin_rs, metric = "rmse")
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
austin_data
data_split <- initial_split(austin_data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
set.seed(123)
austin_data <- train_raw %>%
select(description, priceRange) %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
na.omit()
data_split <- initial_split(austin_data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
price_recipe <- recipe(priceRange ~ description, data = train_data) %>%
step_tokenize(description) %>%
step_stopwords(description) %>%
step_tokenfilter(description, max_tokens = 1000) %>%
step_tf(description) %>%
step_normalize(all_predictors())
model_spec <- linear_reg() %>%
set_engine("lm")
workflow <- workflow() %>%
add_recipe(price_recipe) %>%
add_model(model_spec)
fitted_workflow <- workflow %>%
fit(data = train_data)
predictions <- fitted_workflow %>%
predict(new_data = test_data) %>%
bind_cols(test_data)
metrics <- predictions %>%
metrics(truth = priceRange, estimate = .pred)
print(metrics)
model <- fitted_workflow %>%
extract_fit_parsnip()
tidy(model)
model_spec_rf <- rand_forest(mtry = 5, trees = 500, min_n = 5) %>%
set_engine("randomForest") %>%
set_mode("regression")
workflow_rf <- workflow() %>%
add_recipe(price_recipe) %>%
add_model(model_spec_rf)
fitted_workflow_rf <- workflow_rf %>%
fit(data = train_data)
model_spec_rf <- rand_forest(mtry = 5, trees = 500, min_n = 5) %>%
set_engine("randomForest") %>%
set_mode("regression")
workflow_rf <- workflow() %>%
add_recipe(price_recipe) %>%
add_model(model_spec_rf)
fitted_workflow_rf <- workflow_rf %>%
fit(data = train_data)
predictions_rf <- fitted_workflow_rf %>%
predict(new_data = test_data) %>%
bind_cols(test_data)
metrics_rf <- predictions_rf %>%
metrics(truth = priceRange, estimate = .pred)
print(metrics_rf)
model_spec_rf <- rand_forest(mtry = 5, trees = 500, min_n = 5) %>%
set_engine("randomForest") %>%
set_mode("regression")
workflow_rf <- workflow() %>%
add_recipe(price_recipe) %>%
add_model(model_spec_rf)
fitted_workflow_rf <- workflow_rf %>%
fit(data = train_data)
predictions_rf <- fitted_workflow_rf %>%
predict(new_data = test_data) %>%
bind_cols(test_data)
metrics_rf <- predictions_rf %>%
metrics(truth = priceRange, estimate = .pred)
print(metrics_rf)
cv_folds <- vfold_cv(train_data, v = 5)
grid <- grid_regular(mtry(range = c(3, 10)), trees(range = c(200, 1000)), levels = 5)
tuned_workflow <- workflow() %>%
add_recipe(price_recipe) %>%
add_model(rand_forest(mtry = tune(), trees = tune(), min_n = 5) %>%
set_engine("randomForest") %>%
set_mode("regression"))
tuned_results <- tuned_workflow %>%
tune_grid(resamples = cv_folds, grid = grid, metrics = metric_set(rmse, rsq, mae))
##########
library(tidyverse)
library(tidytext)
library(tidymodels)
train_raw <- read.csv("data/train.csv")
train_raw <- as_tibble(train_raw)
train_raw
austin_tidy <- train_raw %>%
mutate(priceRange = parse_number(priceRange) + 100000) %>%
unnest_tokens(word, description) %>%
anti_join(get_stopwords())
top_words <- austin_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(1:5),
,!word %>% str_detect("\\d")) %>%
slice_max(n, n = 100) %>%
pull(word)
word_freqs <- austin_tidy %>%
count(word, priceRange) %>%
complete(word, priceRange, fill = list(n = 0)) %>%
group_by(priceRange) %>%
mutate(
price_total = sum(n),
proportion = n / price_total
) %>%
ungroup() %>%
filter(word %in% top_words)
word_freqs
word_mods <- word_freqs %>%
nest(data = c(priceRange, n, price_total, proportion)) %>%
mutate(
model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., family = "binomial")),
model = map(model, tidy)
) %>%
unnest(model) %>%
filter(term == "priceRange") %>%
mutate(p.value = p.adjust(p.value)) %>%
arrange(-estimate)
word_mods
higher_words <- word_mods %>%
filter(p.value < 0.05) %>%
slice_max(estimate, n = 30) %>%
pull(word)
lower_words <- word_mods %>%
filter(p.value < 0.05) %>%
slice_max(-estimate, n = 30) %>%
pull(word)
set.seed(123)
austin_split <- train_raw %>%
select(-city) %>%
mutate(description = str_to_lower(description)) %>%
initial_split(strata = priceRange)
austin_train <- training(austin_split)
austin_test <- testing(austin_split)
austin_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)
set.seed(234)
austin_folds <- vfold_cv(austin_train, v = 5, strata = priceRange)
austin_folds
higher_pat <- glue::glue_collapse(higher_words, sep = "|")
lower_pat <- glue::glue_collapse(lower_words, sep = "|")
austin_rec <-
recipe(priceRange ~ ., data = austin_train) %>%
update_role(uid, new_role = "uid") %>%
step_regex(description, pattern = higher_pat, result = "high_price_words") %>%
step_regex(description, pattern = lower_pat, result = "low_price_words") %>%
step_rm(description) %>%
step_novel(homeType) %>%
step_unknown(homeType) %>%
step_other(homeType, threshold = 0.02) %>%
step_dummy(all_nominal_predictors()) %>%
step_nzv(all_predictors())
austin_rec
xgb_spec <-
boost_tree(
trees = 1000,
tree_depth = tune(),
min_n = tune(),
mtry = tune(),
sample_size = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_word_wf <- workflow(austin_rec, xgb_spec)
set.seed(123)
xgb_grid <- grid_max_entropy(
tree_depth(c(5L, 10L)),
min_n(c(10L, 40L)),
mtry(c(5L, 10L)),
sample_prop(c(0.5, 1.0)),
learn_rate(c(-2, -1)),
size = 20
)
xgb_grid
library(finetune)
doParallel::registerDoParallel()
set.seed(234)
xgb_word_rs <-
tune_race_anova(
xgb_word_wf,
austin_folds,
grid = xgb_grid,
metrics = metric_set(mn_log_loss),
control = control_race(verbose_elim = TRUE)
)
library(shiny); runApp('delete.R')
runApp()
runApp()
runApp()
