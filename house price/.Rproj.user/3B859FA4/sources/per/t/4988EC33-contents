---
title: "House price"
author: "edgar M"
date: "2024-04-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(tidyverse)
library(tidytext)

train_raw <- read.csv("data/train.csv")

price_plot <- train_raw %>%
  mutate(priceRange = parse_number(priceRange)) %>%
  ggplot(aes(longitude, latitude, z = priceRange)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs( fill = "mean", title = "Price" )

price_plot

```


```{r}
library(patchwork)

plot_austin <- function(var, title)
  train_raw %>%
  ggplot(aes(longitude, latitude, z = {{var}} )) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs( fill = "mean", title = title )

(price_plot + plot_austin(avgSchoolRating, "School Rating")) /
  (plot_austin(yearBuilt, "Year Built") + plot_austin(log(lotSizeSqFt), "Lot Size"))

```


```{r}
austin_tidy <- train_raw %>%
  mutate(priceRange = parse_number(priceRange) + 100000) %>%
  unnest_tokens(word, description) %>%
  anti_join(get_stopwords())

austin_tidy %>%
  count(word, sort = TRUE)

austin_tidy
```

```{r}
top_words <- austin_tidy %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:5)) %>%
  slice_max(n, n = 100) %>%
  pull(word)

word_freq <- 
  austin_tidy %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>%
  mutate(price_total = sum(n), 
         propotion = n / price_total) %>%
  ungroup() %>%
  filter(word %in% top_words)
 
word_freq
```



```{r}
library(tidyverse)
word_mods <- word_freq %>%
  nest(data = -word) %>%
  mutate(model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., 
                                 family = "binomial")),
         model = map(model, tidy)) %>%
  unnest(model) %>%
  filter(term == "priceRange") %>%
  mutate(p.value = p.adjust(p.value)) %>%
  arrange(estimate)


word_mods
```


```{r}
higher_words <- word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(estimate , n = 10) %>%
  pull(word)

lower_words <- word_mods %>%
  filter(p.value < 0.05) %>%
  slice_max(-estimate , n = 10) %>%
  pull(word)
```



```{r}
word_freq %>%
  filter(word %in% higher_words) %>%
  ggplot(aes(priceRange, propotion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free_y") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "proportion of total words used for homes at high price") +
  theme_light()
  
```




```{r}
word_freq %>%
  filter(word %in% lower_words) %>%
  ggplot(aes(priceRange, propotion, color = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free_y") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(x = NULL, y = "proportion of total words used for homes at lower price") 
```



```{r}

set.seed(123)

austin_split <- train_raw %>%
  select (-city) %>%
  mutate(description = str_to_lower(description)) %>%
  initial_split(strata = priceRange)

austin_split

austin_test <- testing(austin_split)
austin_train <- training(austin_split)
```

```{r}
set.seed(234)

austin_folds <- vfold_cv(austin_train, v =5 , strata = priceRange)
```


```{r}
## feature engineering


higher_pat <- glue::glue_collapse(higher_words, sep = "|")
lower_pat <- glue::glue_collapse(lower_words, sep = "|")


austin_recipe <- recipe(priceRange ~ ., data = austin_train) %>%
  update_role(uid, new_role = "uid") %>%
  step_regex(description, pattern = higher_pat, result = "high_price_words") %>%
  step_regex(description, pattern = lower_pat, result = "low_price_words") %>%
  step_rm(description) %>%
  step_novel(homeType) %>%
  step_unknown(homeType) %>%
  step_other(homeType, threshold = 0.02) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_nzv(all_predictors())
  
austin_recipe
```
```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  mtry = tune(),
  sample_size = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
  
xgb_spec

xgb_word_wf <- workflow(austin_recipe, xgb_spec)


```

```{r}
set.seed(123)
xgb_grid <- grid_max_entropy(
    tree_depth(c(5L, 10L)),
    min_n(c(10L, 40L)),
    mtry(c(5L, 10L)),
    sample_prop(c(0.5, 1.0)),
    learn_rate(c(-2, -1)),
    size = 20
  )

xgb_grid
```

```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(234)
xgb_word_rs <- tune_race_anova(
    xgb_word_wf,
    austin_folds,
    grid = xgb_grid,
    metrics = metric_set(mn_log_loss),
    control = control_race(verbose_elim = TRUE)
  )

xgb_word_rs

```

```{r}
xgb_word_rs <- readRDS("xgb_word_rs_model.rds")
```

```{r}
xgb_word_rs %>%
  collect_metrics()
```



















```{r}
#plot results 
plot_race(xgb_word_rs)
```

Predicting PriceRange using words from description

Top Words
```{r}
top_words <- austin_tidy %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:100000),
         !word %>% str_detect("\\d"),
         !word == "w") %>%
  slice_max(n, n = 500) %>%
  pull(word)

top_words
```


```{r}
word_freq <- austin_tidy %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>%
  mutate(price_total = sum(n), 
         propotion = n / price_total) %>%
  ungroup() %>%
  filter(word %in% top_words)

word_freq
```

```{r}
word_word_cloud <- word_freq %>%
  filter(word %in% higher_words)  %>%
  filter(priceRange == 750000) %>%
  select(-priceRange, -price_total, -propotion) %>%
  mutate(class = "high_words") %>%
  rbind(
    word_freq %>%
      filter(word %in% lower_words)  %>%
      filter(priceRange == 100000) %>%
      select(-priceRange, -price_total, -propotion) %>%
      mutate(class = "low_words")
  )

word_word_cloud
```

```{r}
word_word_cloud %>%
  mutate(class = factor(class)) %>%
  ggplot(aes(label = word, color = class, size = n)) +
  geom_text_wordcloud_area() +
  facet_wrap(~class) +
  scale_size_area(max_size = 16)
```

Processing data for modeling

```{r}
austin_data <- train_raw %>%
  select(description, priceRange) %>%
  mutate(priceRange = parse_number(priceRange) + 100000) %>%
  na.omit()
```








